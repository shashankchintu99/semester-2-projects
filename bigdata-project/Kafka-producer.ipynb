{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a300754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 50000 messages.\n",
      "Sent 100000 messages.\n",
      "Sent 150000 messages.\n",
      "Sent 200000 messages.\n",
      "Sent 250000 messages.\n",
      "Sent 300000 messages.\n",
      "Sent 350000 messages.\n",
      "Sent 400000 messages.\n",
      "Sent 450000 messages.\n",
      "Sent 500000 messages.\n",
      "Sent 550000 messages.\n",
      "Sent 600000 messages.\n",
      "Sent 650000 messages.\n",
      "Sent 700000 messages.\n",
      "Sent 750000 messages.\n",
      "Sent 800000 messages.\n",
      "Data streamed to Kafka successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "def read_json_from_s3(bucket_name, json_object_key, aws_access_key_id, aws_secret_access_key):\n",
    "    try:\n",
    "        s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=json_object_key)\n",
    "        json_data = response['Body'].read().decode('utf-8')\n",
    "        return json.loads(json_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON from S3: {e}\")\n",
    "        return None\n",
    "\n",
    "def stream_to_kafka(json_data, kafka_bootstrap_servers, kafka_topic, batch_size=50000):\n",
    "    try:\n",
    "        producer_config = {\n",
    "            'bootstrap.servers': kafka_bootstrap_servers,\n",
    "            'queue.buffering.max.messages': 1000000,\n",
    "            'acks': 'all',  # Use 'all' for synchronous or -1 for asynchronous\n",
    "        }\n",
    "\n",
    "        producer = Producer(producer_config)\n",
    "        \n",
    "        count = 0  # Counter for batch size\n",
    "        for record in json_data:\n",
    "            # Convert the dictionary to a JSON string\n",
    "            json_record = json.dumps(record)\n",
    "            # Send the record to the Kafka topic\n",
    "            producer.produce(kafka_topic, json_record.encode('utf-8'))\n",
    "            count += 1\n",
    "\n",
    "            # Check if batch size is reached\n",
    "            if count % batch_size == 0:\n",
    "                producer.flush()\n",
    "                print(f\"Sent {count} messages.\")\n",
    "\n",
    "        producer.flush()  # Flush any remaining messages\n",
    "\n",
    "        print(\"Data streamed to Kafka successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error streaming data to Kafka: {e}\")\n",
    "\n",
    "aws_access_key_id = 'AKIAU54TP37SLG7RRXLZ'\n",
    "aws_secret_access_key = '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'\n",
    "bucket_name = '270test'\n",
    "json_object_key = 'Converted_Data.json'\n",
    "kafka_bootstrap_servers = '54.236.25.64:9092'\n",
    "kafka_topic = 'finalproject1'\n",
    "\n",
    "# Read JSON data from S3\n",
    "json_data = read_json_from_s3(bucket_name, json_object_key, aws_access_key_id, aws_secret_access_key)\n",
    "\n",
    "if json_data is not None:\n",
    "    # Stream data to Kafka\n",
    "    stream_to_kafka(json_data, kafka_bootstrap_servers, kafka_topic)\n",
    "else:\n",
    "    print(\"Failed to read JSON from S3.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
